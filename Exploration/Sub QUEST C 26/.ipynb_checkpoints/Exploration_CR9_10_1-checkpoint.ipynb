{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7789fde8",
   "metadata": {},
   "source": [
    "## 사용 데이터셋\n",
    "- 링크 : https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "- 무응답으로 일관하여 실패한 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b158d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dedb37",
   "metadata": {},
   "source": [
    "https://wikidocs.net/31379\n",
    "\n",
    "### Step 1. 데이터 수집하기\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.  \n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.  \n",
    "- 링크 : https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "터미널 경로 설정  \n",
    "- mkdir -p ~/aiffel/transformer_chatbot/data/  \n",
    "- ln -s ~/data/* ~/aiffel/transformer_chatbot/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb79841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ChatBotData.csv', <http.client.HTTPMessage at 0x7ad3fcedf070>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GitHub에서 데이터 다운로드하기\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "urllib.request.urlretrieve(url, filename=\"ChatBotData.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c408c",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800abae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 데이터 전처리하기\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    questions = data['Q'].apply(preprocess_sentence).tolist()\n",
    "    answers = data['A'].apply(preprocess_sentence).tolist()\n",
    "    return questions, answers\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", r\" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "questions, answers = load_data('ChatBotData.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c722d81",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4d332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33834/1290838500.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  outputs = np.array([([target_start_token] + answer + [target_end_token]) for answer in answers_tokenized])\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 없는 경우를 대비하여 예외 처리 추가\n",
    "if len(questions) == 0 or len(answers) == 0:\n",
    "    raise ValueError(\"데이터가 비어 있습니다. 데이터 파일을 확인해주세요.\")\n",
    "\n",
    "# SubwordTextEncoder를 사용하여 데이터 토큰화\n",
    "subword_encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "def tokenize_and_encode(sentences):\n",
    "    return [subword_encoder.encode(sentence) for sentence in sentences]\n",
    "\n",
    "questions_tokenized = tokenize_and_encode(questions)\n",
    "answers_tokenized = tokenize_and_encode(answers)\n",
    "\n",
    "# 패딩 추가\n",
    "MAX_LENGTH = 40\n",
    "def pad_sequences(tokenized_sentences):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_sentences, maxlen=MAX_LENGTH, padding='post'\n",
    "    )\n",
    "\n",
    "questions_padded = pad_sequences(questions_tokenized)\n",
    "answers_padded = pad_sequences(answers_tokenized)\n",
    "\n",
    "# Train 데이터셋 생성\n",
    "target_start_token = subword_encoder.vocab_size\n",
    "target_end_token = subword_encoder.vocab_size + 1\n",
    "\n",
    "inputs = questions_padded\n",
    "outputs = np.array([([target_start_token] + answer + [target_end_token]) for answer in answers_tokenized])\n",
    "outputs_padded = pad_sequences(outputs)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((inputs, outputs_padded))\n",
    "train_data = train_data.shuffle(len(questions)).batch(64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9f8e6",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef02031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 모델 구성하기\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, units, vocab_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.enc_layers = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        x = self.embedding(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, x, x, attention_mask=mask)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, units, vocab_size, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.dec_layers = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        x = self.embedding(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, enc_output, attention_mask=look_ahead_mask)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a6287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 모델 구성하기\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_layers, units, d_model, num_heads, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, units, vocab_size, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, units, vocab_size, dropout)\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs, targets, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inputs, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, _ = self.decoder(targets, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_size)\n",
    "        return final_output\n",
    "\n",
    "# 트랜스포머 하이퍼파라미터 설정\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "units = 512\n",
    "dropout = 0.1\n",
    "vocab_size = subword_encoder.vocab_size + 2\n",
    "\n",
    "# 트랜스포머 모델 인스턴스 생성\n",
    "transformer = Transformer(vocab_size, num_layers, units, d_model, num_heads, dropout)\n",
    "\n",
    "# # Optimizer 및 손실 함수 설정\n",
    "# learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.9\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad55cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 단계 정의\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = None, None, None\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inp, tar_inp, True, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(tar_real, predictions)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1fd0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1688, Accuracy: 0.0254\n",
      "Epoch 2, Loss: 1.1149, Accuracy: 0.0254\n",
      "Epoch 3, Loss: 1.1138, Accuracy: 0.0255\n",
      "Epoch 4, Loss: 1.1128, Accuracy: 0.0253\n",
      "Epoch 5, Loss: 1.1114, Accuracy: 0.0254\n",
      "Epoch 6, Loss: 1.1108, Accuracy: 0.0256\n",
      "Epoch 7, Loss: 1.1103, Accuracy: 0.0255\n",
      "Epoch 8, Loss: 1.1099, Accuracy: 0.0256\n",
      "Epoch 9, Loss: 1.1096, Accuracy: 0.0256\n",
      "Epoch 10, Loss: 1.1091, Accuracy: 0.0257\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습하기\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_data):\n",
    "        batch_loss = train_step(inp, tar)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    avg_accuracy = train_accuracy.result()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b8c27",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd2c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 모델 평가하기\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [subword_encoder.encode(sentence)]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    output = tf.convert_to_tensor([[subword_encoder.vocab_size]])\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = transformer(inputs, output, False, None, None, None)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        if predicted_id == subword_encoder.vocab_size + 1:\n",
    "            break\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = subword_encoder.decode([i for i in output.numpy()[0] if i < subword_encoder.vocab_size])\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3f836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 안녕하세요. 반갑습니다\n",
      "Bot: \n"
     ]
    }
   ],
   "source": [
    "# 예시 문장 예측\n",
    "def predict(sentence):\n",
    "    response = evaluate(sentence)\n",
    "    print(f'User: {sentence}')\n",
    "    print(f'Bot: {response}')\n",
    "\n",
    "# 대화 예시\n",
    "predict(\"안녕하세요. 반갑습니다\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d31d05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 안녕하세요.\n",
      "Bot: \n"
     ]
    }
   ],
   "source": [
    "# 대화 예시\n",
    "predict(\"안녕하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095c531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
