{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a58c04",
   "metadata": {},
   "source": [
    "## 사용 데이터셋 : KorQuAD(Korean Question Answering Dataset)\n",
    "- KorQuAD(Korean Question Answering Dataset)는 읽기 이해를 위한 데이터셋으로, 질문에 대한 답변이 문맥(Context) 내에서 위치하는 형태. \n",
    "- 일반적인 챗봇의 질문-답변 쌍과는 다소 차이가 있음\n",
    "- 학습 실패... 애초에 질문에 대한 답이 단답이어서 챗봇으로 쓸 수 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0fb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c1c96",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 수집하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704e1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 60407개의 질문과 60407개의 답변이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# KorQuAD JSON 파일 경로\n",
    "file_path = 'KorQuAD_v1.0_train.json'\n",
    "\n",
    "# 데이터 로드\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    korquad = json.load(f)\n",
    "\n",
    "# 질문과 답변 추출\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for data in korquad['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            # 답변은 여러 개일 수 있으나, 첫 번째 것을 사용\n",
    "            answer = qa['answers'][0]['text']\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "print(f'총 {len(questions)}개의 질문과 {len(answers)}개의 답변이 있습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f711fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('미국', 124), ('2009년', 114), ('2008년', 98), ('2010년', 89), ('2007년', 83), ('2005년', 81), ('일본', 78), ('2012년', 78), ('2014년', 76), ('영국', 76)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "answer_counts = Counter(answers)\n",
    "print(answer_counts.most_common(10))  # 상위 10개 빈번한 답변 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fee00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "답변: 교향곡\n",
      "---\n",
      "질문: 바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?\n",
      "답변: 1악장\n",
      "---\n",
      "질문: 바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?\n",
      "답변: 베토벤의 교향곡 9번\n",
      "---\n",
      "질문: 1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?\n",
      "답변: 파우스트\n",
      "---\n",
      "질문: 파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?\n",
      "답변: 합창교향곡\n",
      "---\n",
      "질문: 바그너가 파우스트를 처음으로 읽은 년도는?\n",
      "답변: 1839\n",
      "---\n",
      "질문: 바그너가 처음 교향곡 작곡을 한 장소는?\n",
      "답변: 파리\n",
      "---\n",
      "질문: 바그너의 1악장의 초연은 어디서 연주되었는가?\n",
      "답변: 드레스덴\n",
      "---\n",
      "질문: 바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?\n",
      "답변: 한스 폰 뷜로\n",
      "---\n",
      "질문: 잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?\n",
      "답변: 리스트\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'질문: {questions[i]}')\n",
    "    print(f'답변: {answers[i]}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc64dba",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d04b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    # 한글, 영어, 숫자, 기본적인 특수문자만 남기기\n",
    "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-z0-9?.!,¿ ]+\", \"\", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# 전처리 적용\n",
    "questions = [preprocess_sentence(q) for q in questions]\n",
    "answers = [preprocess_sentence(a) for a in answers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831693a8",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f61a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변을 합쳐서 토크나이저 구축\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# 시작과 종료 토큰 정의\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2  # 시작과 종료 토큰 추가\n",
    "\n",
    "# 인코딩 함수 정의\n",
    "def encode_sentence(sentence):\n",
    "    return START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "\n",
    "# 데이터 인코딩\n",
    "questions_encoded = [encode_sentence(q) for q in questions]\n",
    "answers_encoded = [encode_sentence(a) for a in answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a268ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 토큰화 및 패딩\n",
    "# MAX_LENGTH = 40\n",
    "\n",
    "# def tokenize_and_encode(sentences):\n",
    "#     return [subword_encoder.encode(sentence) for sentence in sentences]\n",
    "\n",
    "# questions_tokenized = tokenize_and_encode(questions)\n",
    "# answers_tokenized = tokenize_and_encode(answers)\n",
    "\n",
    "# def pad_sequences(tokenized_sentences):\n",
    "#     return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         tokenized_sentences, maxlen=MAX_LENGTH, padding='post'\n",
    "#     )\n",
    "\n",
    "# questions_padded = pad_sequences(questions_tokenized)\n",
    "# answers_padded = pad_sequences(\n",
    "#     [START_TOKEN + tokens + END_TOKEN for tokens in answers_tokenized]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1101fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 마스크 생성 함수\n",
    "# def create_padding_mask(seq):\n",
    "#     seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "#     return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# def create_look_ahead_mask(size):\n",
    "#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "#     return mask  # Shape: (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c30dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_masks(inputs, targets):\n",
    "#     enc_padding_mask = create_padding_mask(inputs)\n",
    "#     dec_padding_mask = create_padding_mask(inputs)\n",
    "#     look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "#     look_ahead_mask = tf.maximum(dec_padding_mask, look_ahead_mask)\n",
    "#     return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "\n",
    "# # 데이터셋 준비\n",
    "# def map_fn(inputs, targets):\n",
    "#     enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inputs, targets)\n",
    "#     return (\n",
    "#         inputs,  # Encoder inputs\n",
    "#         targets[:, :-1],  # Decoder inputs\n",
    "#         enc_padding_mask,\n",
    "#         look_ahead_mask,\n",
    "#         dec_padding_mask,\n",
    "#         targets[:, 1:],  # Target outputs\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47acf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 샘플 데이터셋 생성\n",
    "# questions_padded = tf.random.uniform((100, 40), maxval=100, dtype=tf.int32)  # (100, 40)\n",
    "# answers_padded = tf.random.uniform((100, 40), maxval=100, dtype=tf.int32)  # (100, 40)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((questions_padded, answers_padded))\n",
    "# dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# dataset = dataset.shuffle(20000).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# # 모델 정의 (간단한 Transformer 블록 예시)\n",
    "# vocab_size = 1000\n",
    "# d_model = 128\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(None,), name=\"inputs\")\n",
    "# dec_inputs = tf.keras.layers.Input(shape=(None,), name=\"dec_inputs\")\n",
    "# enc_padding_mask = tf.keras.layers.Input(shape=(1, 1, None), name=\"enc_padding_mask\")\n",
    "# look_ahead_mask = tf.keras.layers.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "# dec_padding_mask = tf.keras.layers.Input(shape=(1, 1, None), name=\"dec_padding_mask\")\n",
    "\n",
    "# embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "# x = embedding(inputs)\n",
    "# y = embedding(dec_inputs)\n",
    "\n",
    "# outputs = tf.keras.layers.Dense(vocab_size)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4fe555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 20000\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# try:\n",
    "#     # Ensure questions_padded and answers_padded have consistent shapes\n",
    "#     print(f\"Questions shape: {questions_padded.shape}, Answers shape: {answers_padded.shape}\")\n",
    "    \n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((questions_padded, answers_padded))\n",
    "#     dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     print(\"Dataset successfully created.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error while creating dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd04e0f",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63106bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "944/944 [==============================] - 313s 327ms/step - loss: 0.8628 - accuracy: 0.9058\n",
      "Epoch 2/10\n",
      "944/944 [==============================] - 309s 328ms/step - loss: 0.6856 - accuracy: 0.9138\n",
      "Epoch 3/10\n",
      "944/944 [==============================] - 306s 324ms/step - loss: 0.6571 - accuracy: 0.9139\n",
      "Epoch 4/10\n",
      "944/944 [==============================] - 304s 322ms/step - loss: 0.6330 - accuracy: 0.9143\n",
      "Epoch 5/10\n",
      "944/944 [==============================] - 303s 321ms/step - loss: 0.6215 - accuracy: 0.9145\n",
      "Epoch 6/10\n",
      "944/944 [==============================] - 307s 325ms/step - loss: 0.6141 - accuracy: 0.9148\n",
      "Epoch 7/10\n",
      "944/944 [==============================] - 308s 326ms/step - loss: 0.6086 - accuracy: 0.9151\n",
      "Epoch 8/10\n",
      "944/944 [==============================] - 307s 326ms/step - loss: 0.6036 - accuracy: 0.9153\n",
      "Epoch 9/10\n",
      "944/944 [==============================] - 306s 324ms/step - loss: 0.5991 - accuracy: 0.9156\n",
      "Epoch 10/10\n",
      "944/944 [==============================] - 304s 322ms/step - loss: 0.5952 - accuracy: 0.9158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcafc560d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 올바른 VOCAB_SIZE 설정\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2  # 시작 및 종료 토큰 포함\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "NUM_LAYERS = 2\n",
    "MAX_LENGTH = 40\n",
    "L2_REG = 1e-5  # 정규화 강도 감소\n",
    "DROPOUT_RATE = 0.3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# 패딩\n",
    "questions_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    questions_encoded, maxlen=MAX_LENGTH, padding='post')\n",
    "answers_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    answers_encoded, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'encoder_inputs': questions_padded,\n",
    "        'decoder_inputs': answers_padded[:, :-1]\n",
    "    },\n",
    "    answers_padded[:, 1:]\n",
    "))\n",
    "dataset = dataset.shuffle(len(questions_padded)).batch(BATCH_SIZE)\n",
    "\n",
    "# 모델 정의\n",
    "def transformer_with_regularization(vocab_size, embedding_dim, units, num_layers):\n",
    "    # 입력 레이어\n",
    "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embedding = Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # 인코더\n",
    "    encoder_embedding = embedding(encoder_inputs)\n",
    "    encoder_output = encoder_embedding\n",
    "    for _ in range(num_layers):\n",
    "        # LSTM 레이어에 내부 드롭아웃 적용\n",
    "        encoder_output = LSTM(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            dropout=DROPOUT_RATE,\n",
    "            recurrent_dropout=DROPOUT_RATE\n",
    "        )(encoder_output)\n",
    "\n",
    "    # 디코더\n",
    "    decoder_embedding = embedding(decoder_inputs)\n",
    "    decoder_output = decoder_embedding\n",
    "    for _ in range(num_layers):\n",
    "        # LSTM 레이어에 내부 드롭아웃 적용\n",
    "        decoder_output = LSTM(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            dropout=DROPOUT_RATE,\n",
    "            recurrent_dropout=DROPOUT_RATE\n",
    "        )(decoder_output)\n",
    "\n",
    "    # 출력 레이어\n",
    "    outputs = Dense(vocab_size, activation='softmax', kernel_regularizer=l2(L2_REG))(decoder_output)\n",
    "\n",
    "    # 모델 정의\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "model = transformer_with_regularization(VOCAB_SIZE, EMBEDDING_DIM, UNITS, NUM_LAYERS)\n",
    "\n",
    "# 옵티마이저 설정 (학습률 감소 및 그라디언트 클리핑 적용)\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습\n",
    "model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d96c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_LAYERS = 2\n",
    "# D_MODEL = 256\n",
    "# NUM_HEADS = 8\n",
    "# UNITS = 512\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "# model = tf.keras.Model(\n",
    "#     inputs=[inputs, dec_inputs, enc_padding_mask, look_ahead_mask, dec_padding_mask],\n",
    "#     outputs=outputs,\n",
    "# )\n",
    "\n",
    "# # 손실 함수와 옵티마이저 정의\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "# def loss_function(real, pred):\n",
    "#     mask = tf.cast(tf.not_equal(real, 0), dtype=pred.dtype)\n",
    "#     loss = loss_object(real, pred)\n",
    "#     loss *= mask\n",
    "#     return tf.reduce_mean(loss)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9e1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 루프\n",
    "# EPOCHS = 10\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for batch, (inputs, dec_inputs, enc_padding_mask, look_ahead_mask, dec_padding_mask, targets) in enumerate(dataset):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             predictions = model(\n",
    "#                 [inputs, dec_inputs, enc_padding_mask, look_ahead_mask, dec_padding_mask],\n",
    "#                 training=True\n",
    "#             )\n",
    "#             loss = loss_function(targets, predictions)\n",
    "\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "#         print(f'Epoch {epoch + 1} Batch {batch + 1}, Loss {loss.numpy():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbbec1",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed210077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "실제 답변: 교향곡\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?\n",
      "실제 답변: 1악장\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?\n",
      "실제 답변: 베토벤의 교향곡 9번\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?\n",
      "실제 답변: 파우스트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?\n",
      "실제 답변: 합창교향곡\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너가 파우스트를 처음으로 읽은 년도는?\n",
      "실제 답변: 1839\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너가 처음 교향곡 작곡을 한 장소는?\n",
      "실제 답변: 파리\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너의 1악장의 초연은 어디서 연주되었는가?\n",
      "실제 답변: 드레스덴\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?\n",
      "실제 답변: 한스 폰 뷜로\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?\n",
      "실제 답변: 리스트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?\n",
      "실제 답변: 20루이의 금\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 파우스트 교향곡을 부활시킨 사람은?\n",
      "실제 답변: 리스트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?\n",
      "실제 답변: 한스 폰 뷜로\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 1악장을 부활시켜 연주한 사람은?\n",
      "실제 답변: 리스트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?\n",
      "실제 답변: 한스 폰 뷜로\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 리스트가 바그너와 알게 된 연도는?\n",
      "실제 답변: 1840년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 서주에는 무엇이 암시되어 있는가?\n",
      "실제 답변: 주제, 동기\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?\n",
      "실제 답변: 제1바이올린\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 주요부는 어떤 형식으로 되어 있는가?\n",
      "실제 답변: 소나타 형식\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 첫 부분의 주요주제를 암시하는 주제는?\n",
      "실제 답변: 저음 주제\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 제2주제의 축소된 재현부의 조성은?\n",
      "실제 답변: d장조\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 곡이 시작할때의 박자는?\n",
      "실제 답변: 44박자\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 이 곡의 주요 주제는?\n",
      "실제 답변: 고뇌와 갈망 동기, 청춘의 사랑 동기\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 제 2주제에선 무슨 장조로 재현되는가?\n",
      "실제 답변: d장조\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 모든 구강기관으로 여성의 성기를 애무하는 것을 뭐라고 하는가?\n",
      "실제 답변: 커닐링구스\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 구강성교를 할 때 중요한 감각은 무엇인가?\n",
      "실제 답변: 클리토리스\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 구강기관으로 여성의 성기를 애무하는 행위를 뜻하는 용어는?\n",
      "실제 답변: 커닐링구스\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 구강성교시 특히 어느 부위의 감각이 중요한가?\n",
      "실제 답변: 클리토리스\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 여성의 성기를 애무하는 것을 말하는 커닐링구스를 다른 말로 무엇이라고 합니까?\n",
      "실제 답변: 구강성교\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 커닐링구스는 받는 사람이 다양한 성감을 느끼는 원인이 되며 무엇의 감각이 특히 중요합니까?\n",
      "실제 답변: 클리토리스\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 오르가슴은 클리토리스에 직접적인 자극을 주는 커닐링구스를 통해 쉽게 도달할 수 있다고 여성의 성 보고서를 쓴 사람은 누구인가?\n",
      "실제 답변: 쉐어 하이트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 여성이 커닐링구스를 하기 전에 중요하게 생각하는 것은 무엇인가?\n",
      "실제 답변: 개인 위생\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 일반적 통계에 따르면 오르가슴을 얻기 위해 직접적인 음핵 자극을 필요로 하는 여성은 몇 퍼센트인가?\n",
      "실제 답변: 80\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 대부분의 여성이 커닐링구스를 통해 쉽게 오르가슴에 도달할 수 있다는 성 보고서를 작성한 학자는 누구인가?\n",
      "실제 답변: 쉐어 하이트\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 일반적인 통계로 볼 때, 여성의 80가 오르가슴을 얻으려고 직접적인 어떤 자극을 필요로 합니까?\n",
      "실제 답변: 음핵 자극\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 커닐링구스를 받는 여성은 파트너의 혀가 클리토리스를 잘 자극할 수 있도록 손가락을 사용하면 직접 무엇을 분리할 수 있습니까?\n",
      "실제 답변: 음순\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 성경에서 커닐링구스에 대해 우회적으로 번역한 단어는 무엇인가?\n",
      "실제 답변: 배꼽\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 육육적인 그릇에서 마시고 싶은 남자의 욕망을 암시할 때 여성의 웅덩이에 가득 채워지고 있던 것은 무엇인가?\n",
      "실제 답변: 포도주\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 아가서 중 커닐링구스에 대한 우회적 언급으로 생각할 수 있는 구절의 핵심 단어는 주로 무엇으로 번역되는가?\n",
      "실제 답변: 배꼽\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 아가서 7장 3절에 대한 해석 중 음부를 동그란 컵에 비유한 번역은 컵에 무엇이 끊어진 적이 없다고 하였는가?\n",
      "실제 답변: 칵테일\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 성경의 아가서 73의 핵심 단어를 많은 번역자들이 무엇으로 번역했습니까?\n",
      "실제 답변: 배꼽\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기독교와 유대교의 전통에서 아가에 그려진 신랑과 신부의 에로틱한 친밀감에 어떤 의미를 부여하고 있습니까?\n",
      "실제 답변: 영적인 의미\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선은 오사카 덴노지에서 어디까지 가는 노선인가?\n",
      "실제 답변: 와카야마\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 심한 구배 구간이 존재하는 야마나카다니역기이 역 사이에 있는 고개는?\n",
      "실제 답변: 오노야마 고개\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선이 기세이 본선과 접속하는 역은?\n",
      "실제 답변: 와카야마 역\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 국제공항이 열린 해는?\n",
      "실제 답변: 1994년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선의 종착점은?\n",
      "실제 답변: 와카야마\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 와카야마 역에서 난키 방면으로 운행하고 있는 것은?\n",
      "실제 답변: 특급 열차\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 국제공항이 개통된 해는?\n",
      "실제 답변: 1994년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 덴노지  히네노 역간에 대낮 시간에 모두 시간당 12편이 운행하는데 어떤 종류의 열차들인가?\n",
      "실제 답변: 쾌속과 보통\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 히네노 역 이남 구간과 이북 구간 중 열차 편수가 더 많은 곳은 어디인가?\n",
      "실제 답변: 이북 구간\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 본선 jr 난바 역 발착 열차는 2008년 3월 이 일이 있은 후 없어졌다. 이 일은?\n",
      "실제 답변: 시간표 개정\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 오사카 방면에서 출발한 대부분의 열차가 종착하는 역은?\n",
      "실제 답변: 히네노 역\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 본선 jr 난바 역 발착의 열차가 폐지된 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 오사카 방면에서 온 대부분의 열차가 종착하는 곳은?\n",
      "실제 답변: 히네노 역\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 히네노 역 이남 지역에서 우선도가 높은 열차는?\n",
      "실제 답변: 특급\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 본선 jr 난바역 발착 열차가 존재했던 마지막 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 덴노지 역에서 2008년 3월 까지 진행된 공사로 야마토지 선과 한와 선과의 교차를 단락선에서 어떻게 바뀌었나?\n",
      "실제 답변: 복선화\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 히네노 역 이북에 증편된 쾌속 열차는 모두 몇 량인가?\n",
      "실제 답변: 8량\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 야마토지 선의 시간표가 더 간단해지게 된 이유는 무엇일까?\n",
      "실제 답변: 복선화\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선이 덴노지 역에서 평면 교차하는 노선은?\n",
      "실제 답변: 야마토지 선\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 오사카 순환선 각역에 정차하는 한와 선 직통 쾌속이 개시된 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 복선화가 시간표 혼란을 막은 선은?\n",
      "실제 답변: 야마토지 선\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 복선화 이후 하네노 역 이북에서 대폭 증가한 쾌속 열차는 몇 량으로 구성되었는가?\n",
      "실제 답변: 8량\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간사이 공항선에 직통하는 특급 열차는 무엇인가?\n",
      "실제 답변: 하루카 호\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기세이본선과 간사이 공항선에 직통하는 열차가 운행되는 정차역들 중 오토리에서 제외되는 역은 오션애로우와 어디인가?\n",
      "실제 답변: 하루카\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와선 은 승객이 많을 경우 9량으로 편성되기도 하는데 기본적으로 편성운행되는 열차는 몇량일까?\n",
      "실제 답변: 6량\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 특급 하루카 호가 직통하고 있는 선은?\n",
      "실제 답변: 간사이 공항선\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 승객이 많은 시기에 운전되는 것은?\n",
      "실제 답변: 임시 열차\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기본적으로 6량 편성이지만 승객이 많을 때 운행되는 량 수는?\n",
      "실제 답변: 9량\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와선의 전신은 1984년 운전개시한 이열차인데 이름은?\n",
      "실제 답변: 홈 라이너 이즈미\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선의 전신인 열차가 1986년 11월이후 불리우던 이름은?\n",
      "실제 답변: 한와 라이너\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 라이너 열차 내에 금연화가 시행된 날짜는 2009년 언제 부터인가?\n",
      "실제 답변: 6월 1일\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 라이너가 도입된 해는?\n",
      "실제 답변: 1986년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 홈 라이너 이즈미가 개시된 해는?\n",
      "실제 답변: 1984년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 홈 라이너 이즈미가 1986년 바뀐 이름은?\n",
      "실제 답변: 한와 라이너\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 홈 라이너 이즈미가 1986년 운전 구간이 연장된 역은?\n",
      "실제 답변: 와카야마 역\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 라이너가 휴일 시간대 운전이 중단된 해는?\n",
      "실제 답변: 2009년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간쿠 쾌속열차는 낮시간대에 간사이공항역을 출발해 어디까지 운행되는가?\n",
      "실제 답변: 교바시\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 조조 및 심야시간대를 제외한 히네노역 이북 전 구간에 운행되는 열차는 몇량으로 편성 되나?\n",
      "실제 답변: 8량\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 2008년 3월 시간표 개정 이후 추가정차하게 된 역은?\n",
      "실제 답변: 덴마 역과 사쿠라노미야\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간쿠 쾌속이 개시된 해는?\n",
      "실제 답변: 1994년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 특별쾌속 윙 호가 사라진 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간쿠 쾌속과 함께 간사이 국제공항 연계 열차인 것은?\n",
      "실제 답변: 간사이 공항 특급 하루카\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 간쿠 쾌속이 덴마 역과 사쿠라노미야 역에 정차하게 된 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 2008년 3월 15일에 소멸한 특별쾌속의 이름은?\n",
      "실제 답변: 윙\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 1999년 5월 와카야마 방면으로 신설된 특급열차는?\n",
      "실제 답변: 기슈지 쾌속\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기슈지 쾌속의 시발지은 대부분 어느 곳인가?\n",
      "실제 답변: 교바시\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기슈지 쾌속이 생겨난 해는?\n",
      "실제 답변: 1999년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기슈지 쾌속이 발착하는 노선은?\n",
      "실제 답변: 오사카 순환선\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 1999년 5월부터 신설된 쾌속 열차는?\n",
      "실제 답변: 기슈지 쾌속\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 기슈지 쾌속의 시발역은?\n",
      "실제 답변: 와카야마 역\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 오사카역 교바시 방면의 직통쾌속 상행선은 모두 몇편인가?\n",
      "실제 답변: 10편\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 하행 직통쾌속은 오토리역, 와카야마역행 모두 몇편 운행되나?\n",
      "실제 답변: 각각 1편씩\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선 직통 쾌속이 생겨난 해는?\n",
      "실제 답변: 2008년\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 한와 선 직통 쾌속이 각역에 정차하는 노선은?\n",
      "실제 답변: 오사카 순환선\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 2008년 평일 아침 출근 시간대에 신설된 오사카 순환선 직통 열차의 이름은?\n",
      "실제 답변: 직통 쾌속\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 직통 쾌속이 사용하는 차량은?\n",
      "실제 답변: 223계\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 대낮 시간대에 덴노지역히네노.와카야미역 간에는 열차가 몇편 운행되나?\n",
      "실제 답변: 시간당 3편씩\n",
      "예측 답변: 이동\n",
      "---\n",
      "질문: 후속열차의 쾌속, 특급열차의 통과를 위해 있어야 하는 설비는?\n",
      "실제 답변: 대피 설비\n",
      "예측 답변: 이동\n",
      "---\n",
      "BLEU 스코어 평균: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# 답변 생성 함수\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence_encoded = encode_sentence(sentence)\n",
    "    sentence_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sentence_encoded], maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    decoder_input = START_TOKEN  # START_TOKEN 자체를 사용\n",
    "    output = tf.expand_dims(decoder_input, 0)  # shape: [1, 1]\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model.predict([sentence_padded, output])\n",
    "        predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
    "\n",
    "        if predicted_id == END_TOKEN[0]:\n",
    "            break\n",
    "\n",
    "        # output과 predicted_id의 차원을 맞춰서 concat\n",
    "        output = tf.concat([output, [[predicted_id]]], axis=-1)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in output.numpy()[0] if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    return predicted_sentence\n",
    "\n",
    "# 예시 문장에 대한 답변 생성 및 BLEU 스코어 계산\n",
    "reference_answers = []\n",
    "predicted_answers = []\n",
    "\n",
    "for i in range(100):  # 테스트 데이터 중 100개 사용\n",
    "    question = questions[i]\n",
    "    real_answer = answers[i]\n",
    "    predicted_answer = evaluate(question)\n",
    "\n",
    "    reference_answers.append([real_answer.split()])\n",
    "    predicted_answers.append(predicted_answer.split())\n",
    "\n",
    "    print(f'질문: {question}')\n",
    "    print(f'실제 답변: {real_answer}')\n",
    "    print(f'예측 답변: {predicted_answer}')\n",
    "    print('---')\n",
    "\n",
    "# BLEU 스코어 계산\n",
    "bleu_scores = [\n",
    "    sentence_bleu(ref, pred, weights=(0.5, 0.5))  # BLEU-2 사용\n",
    "    for ref, pred in zip(reference_answers, predicted_answers)\n",
    "]\n",
    "average_bleu = np.mean(bleu_scores)\n",
    "print(f'BLEU 스코어 평균: {average_bleu}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ad6a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge-score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge-score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge-score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (4.62.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=4dad0365f341210eb339c9e750ea1ce9380b779f255556939d2b79a1eee19faf\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# rouge-score를 사용해보기\n",
    "# 관련 링크 https://velog.io/@jochedda/Rouge-Score-Text-Summarization%EC%9D%98-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C\n",
    "# https://www.youtube.com/watch?v=TMshhnrEXlg\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0353d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score: 0.0000\n",
      "ROUGE-L Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, hyp in zip(real_answer, predicted_answer):\n",
    "    scores = scorer.score(ref, hyp)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "avg_rouge1 = np.mean(rouge1_scores)\n",
    "avg_rougeL = np.mean(rougeL_scores)\n",
    "\n",
    "print(f'ROUGE-1 Score: {avg_rouge1:.4f}')\n",
    "print(f'ROUGE-L Score: {avg_rougeL:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예시 문장 예측\n",
    "# def predict(sentence):\n",
    "#     response = evaluate(sentence)\n",
    "#     print(f'User: {sentence}')\n",
    "#     print(f'Bot: {response}')\n",
    "\n",
    "# # 대화 예시\n",
    "# predict(\"안녕하세요. 반갑습니다\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8b668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
