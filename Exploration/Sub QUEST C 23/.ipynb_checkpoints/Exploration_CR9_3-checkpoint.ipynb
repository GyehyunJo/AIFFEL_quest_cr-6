{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacbbefb",
   "metadata": {},
   "source": [
    "### 3-4 데이터 준비하기\n",
    "우리가 텍스트 요약 모델 학습에 사용할 데이터셋은 Kaggle에서 제공된 아마존 리뷰 데이터셋\n",
    "\n",
    "이번 실습에서는 NLTK의 불용어(stopwords)를 사용할 거에요. NTLK와 NLTK 데이터셋이 설치되어 있지 않은 환경이라면 우선 NLTK를 설치하고 NTLK의 데이터셋을 다운로드해 주세요.\n",
    "NLTK는 Natural Language Toolkit의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러리에요. 이 NLTK에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어 있어요. 이를 이용해 다운로드한 리뷰 파일에서 불용어를 제거하는 작업을 진행할 예정이에요.\n",
    "NLTK 패키지에서 불용어 사전을 다운로드하고, 데이터 전처리를 위한 나머지 패키지도 함께 불러와 볼까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab70439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk = \"Natural Language Toolkit\", 자연어 처리를 위한 Python 라이브러리\n",
    "nltk.download('stopwords')\n",
    "# nltk 라이브러리에서 'stopwords'불용어 데이터 다운로드. 불용어(stopwords)는 분석에 큰 의미가 없는 단어들(예: 영어의 \"the\", \"and\", \"is\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # NLTK의 corpus 모듈에서 불용어 리스트 가져옴\n",
    "from bs4 import BeautifulSoup \n",
    "# BeautifulSoup은 HTML과 XML 문서를 파싱(구문 분석)하는 라이브러리. 보통 웹에서 크롤링한 데이터를 전처리할 때 사용\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "# TensorFlow의 Keras API에서 Tokenizer를 가져옴. 텍스트 데이터를 숫자 토큰으로 변환하는 도구\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# TensorFlow의 Keras API에서 pad_sequences를 가져옴. 텍스트 데이터의 길이를 일정하게 맞춰주는 도구\n",
    "import urllib.request\n",
    "# Python 표준 라이브러리의 urllib.request를 불러옴. URL로부터 데이터를 다운로드하거나 요청을 보낼 때 사용\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "# BeautifulSoup(bs4)에서 발생하는 사용자 경고(UserWarning)를 무시하도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8b4145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "# 10만 개의 샘플만 사용\n",
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cf2cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5개만 출력해서 확인해보기\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2978f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76955</th>\n",
       "      <td>This granola is fantastic.  It's crunchy, ther...</td>\n",
       "      <td>Yummy Yummy in my Tummy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42267</th>\n",
       "      <td>I have modified the instructions to adjust it ...</td>\n",
       "      <td>great curry with some modification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62311</th>\n",
       "      <td>I am an Earl Grey fan, and I am currently enjo...</td>\n",
       "      <td>Good, but mostly for Earl Grey fans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28353</th>\n",
       "      <td>There are no Walnuts in the Berry Nut Blend. I...</td>\n",
       "      <td>No Walnuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73124</th>\n",
       "      <td>Yes, these gluten free cookies are absolutely ...</td>\n",
       "      <td>WOW!  Take a look at the nutritional facts.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19850</th>\n",
       "      <td>I visited my daughter in NC and she had the Ke...</td>\n",
       "      <td>newlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>i love the convenience of the singles. The bes...</td>\n",
       "      <td>coffee-mate coffee creamer hazelnut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84550</th>\n",
       "      <td>I'm from the south and Cold Sweet Iced Tea is ...</td>\n",
       "      <td>My Favorite Black Tea for Sweet Iced Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30385</th>\n",
       "      <td>I have tried several marinades over the years ...</td>\n",
       "      <td>The Best Marinade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41571</th>\n",
       "      <td>I go through a lot of matcha, and I've tried a...</td>\n",
       "      <td>Lots of perfectly acceptable Matcha for absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22605</th>\n",
       "      <td>Like \"Kitten Kisser\", I must warn others to do...</td>\n",
       "      <td>AVOID!  Price Has Doubled!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>There is a reason they serve this coffee at th...</td>\n",
       "      <td>The best coffee ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44949</th>\n",
       "      <td>So my Mom often buys bags of kitty treats for ...</td>\n",
       "      <td>The only treats my cat will eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83082</th>\n",
       "      <td>We purchase on Subscribe and Save, 3 boxes a m...</td>\n",
       "      <td>Robust taste, packaging could be better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41580</th>\n",
       "      <td>Great matcha powder, ideal for making frappes....</td>\n",
       "      <td>Great! Got what I expected!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "76955  This granola is fantastic.  It's crunchy, ther...   \n",
       "42267  I have modified the instructions to adjust it ...   \n",
       "62311  I am an Earl Grey fan, and I am currently enjo...   \n",
       "28353  There are no Walnuts in the Berry Nut Blend. I...   \n",
       "73124  Yes, these gluten free cookies are absolutely ...   \n",
       "19850  I visited my daughter in NC and she had the Ke...   \n",
       "325    i love the convenience of the singles. The bes...   \n",
       "84550  I'm from the south and Cold Sweet Iced Tea is ...   \n",
       "30385  I have tried several marinades over the years ...   \n",
       "41571  I go through a lot of matcha, and I've tried a...   \n",
       "22605  Like \"Kitten Kisser\", I must warn others to do...   \n",
       "10043  There is a reason they serve this coffee at th...   \n",
       "44949  So my Mom often buys bags of kitty treats for ...   \n",
       "83082  We purchase on Subscribe and Save, 3 boxes a m...   \n",
       "41580  Great matcha powder, ideal for making frappes....   \n",
       "\n",
       "                                                 Summary  \n",
       "76955                           Yummy Yummy in my Tummy!  \n",
       "42267                 great curry with some modification  \n",
       "62311                Good, but mostly for Earl Grey fans  \n",
       "28353                                         No Walnuts  \n",
       "73124    WOW!  Take a look at the nutritional facts.....  \n",
       "19850                                            newlife  \n",
       "325                  coffee-mate coffee creamer hazelnut  \n",
       "84550           My Favorite Black Tea for Sweet Iced Tea  \n",
       "30385                                  The Best Marinade  \n",
       "41571  Lots of perfectly acceptable Matcha for absolu...  \n",
       "22605                         AVOID!  Price Has Doubled!  \n",
       "10043                               The best coffee ever  \n",
       "44949                    The only treats my cat will eat  \n",
       "83082           Robust taste, packaging could be better.  \n",
       "41580                        Great! Got what I expected!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text와 Summary 컬럼만 남기기\n",
    "data = data[['Text', 'Summary']]\n",
    "\n",
    "# 변경된 데이터프레임 확인\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a20cb",
   "metadata": {},
   "source": [
    "### 3-5. 데이터 전처리하기 \n",
    "(1) 데이터 정리하기\n",
    "이제 데이터를 불러왔으니 전처리를 진행해 볼게요. 빈칸으로 존재하는 null 데이터, 의미는 같지만 다른 식으로 작성된 글 같은 중복 항목과 같은 학습할 때 방해가 되는 데이터를 먼저 솎아낼 거예요.\n",
    "\n",
    "중복 샘플과 NULL 값이 존재하는 샘플 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41b0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a194f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d1a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff983b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d5e0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "# contractions 사전 정의 : 영어 축약형(예: \"can't\", \"won't\")과 그것의 풀어 쓴 형태(expanded form)를 매핑하는 사전을 정의\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e27badff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 이 코드는 NLTK 라이브러리를 사용하여 영어 불용어(stopwords)의 개수를 출력하고, 실제 불용어 목록을 보여줌\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dfdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15db1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "# 예시용 코드\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1b0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "# 각 텍스트(i)를 전처리 함수 preprocess_sentence를 사용해 처리. 전처리된 결과를 clean_text 리스트에 추가.\n",
    "for i in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(i))\n",
    "\n",
    "# 전처리된 텍스트 중 앞에서 5개를 출력하여 확인합니다\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fbce8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['good quality dog food', 'not as advertised', 'delight says it all', 'cough medicine', 'great taffy']\n"
     ]
    }
   ],
   "source": [
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "# summary는 위 if문에서 else로 가서 불용어를 제거하지 않도록 처리함\n",
    "clean_summary = []\n",
    "\n",
    "for i in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(i, False))\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664c403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 데이터프레임에서 빈 문자열('')을 Null 값(NaN)으로 변환. \n",
    "data.replace('', np.nan, inplace=True) #inplace=True: 데이터프레임을 원본 그대로 변경. Null 값은 데이터 분석 과정에서 결측값 처리를 용이하게 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43967d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인 코드?\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e430157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "# 이 코드는 결측값(NaN)이 포함된 행을 제거한 후, 데이터프레임에 남아있는 샘플 수를 출력\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c34c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ed90892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 예시 코드?\n",
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe35cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0 # 기준 길이 이하인 샘플의 개수를 세기 위한 변수 cnt 초기화\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2832617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409445a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  bought several vitality canned dog food produc...  good quality dog food\n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised\n",
       "2  confection around centuries light pillowy citr...    delight says it all\n",
       "3  looking secret ingredient robitussin believe f...         cough medicine\n",
       "4  great taffy great price wide assortment yummy ...            great taffy"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_max_len과 summary_max_len 길이를 초과하는 샘플 제거\n",
    "data = data[\n",
    "    data['Text'].apply(lambda x: len(x.split()) <= text_max_len) &\n",
    "    data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)\n",
    "]\n",
    "\n",
    "print('전체 샘플수 :', (len(data)))\n",
    "# 변경된 데이터프레임 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836af4e",
   "metadata": {},
   "source": [
    "#### 시작 토큰과 종료 토큰 추가하기\n",
    "앞서 시작 토큰과 종료 토큰에 대해서 언급했던 것을 기억하시나요? 디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈추는 거였죠.\n",
    "\n",
    "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있어요. 이번 실습에서는 시작 토큰은 sostoken, 종료 토큰은 eostoken이라 임의로 명명하고 앞, 뒤로 추가할 거예요. 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 decoder_input, 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 decoder_target이라고 이름을 정했어요. 두 개의 문장 모두 Summary 열로부터 만들 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f9fb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ba0ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f126efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25365  8167 54847 ...  9789  8042 49761]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2f82b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fdeb92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74e9676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116cf8f7",
   "metadata": {},
   "source": [
    "### 3-7. 데이터 전처리하기 \n",
    "(3) 정수 인코딩\n",
    "단어 집합(vocabulary) 만들기 및 정수 인코딩\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꾸어 주어야 해요. 이를 위해서는 각 단어에 고유한 정수를 맵핑하는 작업이 필요해요. 이 과정을 단어 집합(vocabulary) 을 만든다고 표현해요. 훈련 데이터에 대해서 단어 집합을 만들어볼게요. 우선, 원문에 해당되는 encoder_input_train에 대해서 단어 집합을 만들게요.\n",
    "\n",
    "Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55a45d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcb0c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 31896\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23650\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8246\n",
      "단어 집합에서 희귀 단어의 비율: 74.14722849260096\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.377747181337992\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ef81",
   "metadata": {},
   "source": [
    "encoder_input_train에는 3만여 개의 단어가 있네요. 그 아래의 통계 정보들을 해석해볼까요?\n",
    "\n",
    "등장 빈도가 threshold 값인 7회 미만, 즉 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지하네요. 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않아요.\n",
    "\n",
    "그래서 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하고자 합니다. 위에서 이를 제외한 단어 집합의 크기를 8천여 개로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8,000으로 제한해볼게요. 토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa4d690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31165ef6",
   "metadata": {},
   "source": [
    "texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행해요. 현재 단어 집합의 크기를 8,000으로 제한했으니까 이제 8,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않아요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95f72d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4790, 295, 653, 455, 1825, 378, 7, 15, 74, 9, 105, 687, 29, 63, 126, 1632, 3522, 130, 2297, 146, 85, 51, 57, 840, 1511, 378, 863, 6, 17, 2, 477, 733, 39, 792, 140, 465, 146, 105, 30, 22, 104, 126], [360, 452, 378, 107, 47, 6, 155, 742, 1, 319, 146, 155, 53, 340, 1043, 3251, 99, 37, 229, 279, 146, 137, 146, 2603, 229, 2095, 229, 2318, 2185, 654, 457, 849, 278, 297, 75, 458, 466, 1289, 271, 220, 16, 14], [59, 5, 5447, 471, 120, 29, 291, 28, 172, 1633, 4, 1, 301, 215, 1768, 1241, 487, 23, 657]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d0d407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bac0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10485\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8107\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2378\n",
      "단어 집합에서 희귀 단어의 비율: 77.31998092513113\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.897650067596077\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e8071",
   "metadata": {},
   "source": [
    "등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지하고 있네요. 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 5.87%밖에 되지 않아요. 아까 했던 것과 동일하게 이 단어들은 모두 제거할게요. 어림잡아 2,000을 단어 집합의 크기로 제한할게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b619ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 351, 61, 49, 211], [1, 17, 10, 49], [1, 21, 77, 10, 20, 236], [1, 41], [1, 50, 47, 856, 11]]\n",
      "target\n",
      "decoder  [[351, 61, 49, 211, 2], [17, 10, 49, 2], [21, 77, 10, 20, 236, 2], [41, 2], [50, 47, 856, 11, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292bd89",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩 작업이 끝났어요. 현재 decoder_input_train과 decoder_target_train에는 더 이상 숫자 2,000이 넘는 숫자들은 존재하지 않아요. 그런데 다음 작업인 패딩 하기로 넘어가기 전에 한 가지 점검해야 할 것이 있어요.\n",
    "\n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있어요. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높겠죠.\n",
    "\n",
    "요약문에서 길이가 0이 된 샘플들의 인덱스를 받아와볼게요. 여기서 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않아요. 그래서 이제 길이가 0이 된 요약문의 실제 길이는 1로 나올 거예요. 길이 0이 된 decoder_input에는 sostoken, decoder_target에는 eostoken만 남아 있을 테니까요.\n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장해볼게요. 이 샘플들은 모두 삭제할 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b862f30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1258\n",
      "삭제할 테스트 데이터의 개수 : 356\n",
      "훈련 데이터의 개수 : 51397\n",
      "훈련 레이블의 개수 : 51397\n",
      "테스트 데이터의 개수 : 12807\n",
      "테스트 레이블의 개수 : 12807\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6ca3b",
   "metadata": {},
   "source": [
    "#### 패딩하기\n",
    "\n",
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해주어야 해야 해요. 아까 정해두었던 최대 길이로 패딩 해 줄 거에요. 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3cae6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# padding: pre 또는 post를 선택. (post는 뒤쪽에 패딩, pre는 앞쪽에 패딩)\n",
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a6dc5",
   "metadata": {},
   "source": [
    "### 3-8. 모델 설계하기\n",
    "이제는 모델을 설계할 시간이에요. 우선 함수형 API를 이용해서 인코더를 설계해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c2e502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072e119",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요. hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴런의 개수라고 이해하면 돼요. 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요.\n",
    "\n",
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였어요. hidden state의 크기를 늘리는 것이 LSTM 층 1개의 용량을 늘린다면, 3개의 층을 사용하는 것은 모델의 용량을 늘린다고 볼 수 있죠. 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내줘야겠죠?\n",
    "\n",
    "또한 LSTM은 dropout 뿐 아니라 recurrent dropout까지 사용할 수 있어요. 일반적인 dropout은 레이어의 weight를 랜덤으로 생략하여 모델의 과적합(overfitting)을 해결해주는 방법이에요.\n",
    "\n",
    "반면 recurrent dropout은 dropout을 레이어가 아닌 time step마다 해주는 방식이에요. 즉 time step의 입력을 랜덤으로 생략해 주는 거죠. recurrent dropout은 일반적인 dropout와 같이 regularization을 해주는 효과가 있고, 과적합을 방지할 수 있다고 해요.\n",
    "\n",
    "아래 그림은 일반적인 dropout과, dropout과 recurrent dropout을 동시에 사용한 것을 시각적으로 표현한 것입니다. 색이 있는 화살표는 dropout을 나타낸 것이에요. (색이 다른 것은 다른 dropout mask를 사용했다는 표시인데, 지금은 그냥 넘어가셔도 됩니다.) 코드를 수정해서 LSTM에 dropout과 recurrent dropout을 모두 사용할 수 있습니다. 그렇게 되면 오른쪽 그림과 같은 형태가 되겠군요. 참고로 dropout과 recurrent dropout을 모두 사용한 것을 Variational Dropout이라고도 해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "262cdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef27c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 50, 128)      1024000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 50, 256), (N 394240      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 50, 256), (N 525312      lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 128)    256000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 50, 256), (N 525312      lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, None, 256),  394240      embedding_5[0][0]                \n",
      "                                                                 lstm_10[0][1]                    \n",
      "                                                                 lstm_10[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   514000      lstm_11[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bda2599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 50, 128)      1024000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 50, 256), (N 394240      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 50, 256), (N 525312      lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 128)    256000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 50, 256), (N 525312      lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, None, 256),  394240      embedding_5[0][0]                \n",
      "                                                                 lstm_10[0][1]                    \n",
      "                                                                 lstm_10[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_11[0][0]                    \n",
      "                                                                 lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_11[0][0]                    \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e71f8",
   "metadata": {},
   "source": [
    "디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 해요. 그렇기 때문에 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용하고 있어요.\n",
    "\n",
    "지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq에요. 그런데 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법이 있어요! 바로 어텐션 메커니즘이에요.\n",
    "\n",
    "#### 어텐션 메커니즘\n",
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야 한다는 뜻이에요. 어텐션 함수를 설계해보는 것은 다음 기회로 미루기로 하고, 여기서는 TensorFlow에 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 배워볼게요. 참고로 여기서 사용하는 어텐션 함수는 Bahdanau 스타일의 어텐션입니다. 이 어텐션에 대한 자세한 설명은 텐서플로우 홈페이지를 참고하세요.\n",
    "\n",
    "아래와 같이 어텐션 층을 만들고, 위에서 설계한 디코더의 출력층을 수정해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60866f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 50, 128)      1024000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 50, 256), (N 394240      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 50, 256), (N 525312      lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 128)    256000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 50, 256), (N 525312      lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, None, 256),  394240      embedding_5[0][0]                \n",
      "                                                                 lstm_10[0][1]                    \n",
      "                                                                 lstm_10[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_11[0][0]                    \n",
      "                                                                 lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_11[0][0]                    \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ec5f1",
   "metadata": {},
   "source": [
    "### 3-9. 모델 훈련하기\n",
    "설계한 모델을 가지고 훈련을 진행해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9f2e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 54s 147ms/step - loss: 2.7069 - val_loss: 2.4232\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 29s 145ms/step - loss: 2.3713 - val_loss: 2.2708\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 2.2209 - val_loss: 2.1518\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 2.1009 - val_loss: 2.0719\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 2.0196 - val_loss: 2.0188\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 1.9546 - val_loss: 1.9723\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.9032 - val_loss: 1.9464\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 1.8575 - val_loss: 1.9245\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.8180 - val_loss: 1.9096\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.7808 - val_loss: 1.8972\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 1.7467 - val_loss: 1.8844\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.7143 - val_loss: 1.8716\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.6858 - val_loss: 1.8647\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.6560 - val_loss: 1.8639\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 1.6291 - val_loss: 1.8665\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 30s 150ms/step - loss: 1.6027 - val_loss: 1.8636\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.5794 - val_loss: 1.8625\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 30s 149ms/step - loss: 1.5545 - val_loss: 1.8651\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 14s 71ms/step - loss: 1.5324 - val_loss: 1.8674\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)\n",
    "# LMS에서는 18번째에서, 여기서는 19번째에서 early stopping 뜸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064dc7e",
   "metadata": {},
   "source": [
    "'조기 종료'를 뜻하는 EarlyStopping은 특정 조건이 충족되면 훈련을 멈추는 역할을 해요.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "위 코드에서는 val_loss(검증 데이터의 손실)을 관찰하다가, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정돼 있어요. EarlyStopping이 작동한다면 epochs가 아무리 크게 설정되어 있어도 모델 훈련을 최적점에서 멈출 수 있겠네요.\n",
    "\n",
    "EarlyStopping에 대한 자세한 내용은 아래 링크를 참고하세요! 자주 쓰이는 도구이니 자세히 알아두면 매우 도움이 될 거예요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcb2171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b5ace",
   "metadata": {},
   "source": [
    "### 3-10. 인퍼런스 모델 구현하기\n",
    "테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비해 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9934ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce2e48",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다는 것, 알고 계시나요?\n",
    "\n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한 번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했습니다.\n",
    "\n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 합니다. 이때는 인코더 모델과 디코더 모델을 분리해서 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7ca467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a43de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8b081e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e261e2",
   "metadata": {},
   "source": [
    "Q. 정답 문장이 없는 추론(inference) 단계에서는 왜 모델 설계를 별도로 해주어야 하나요?\n",
    "A. 생성해야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b18fd",
   "metadata": {},
   "source": [
    "### 3-11. 모델 테스트하기\n",
    "테스트 단계에서는 정수 시퀀스를 텍스트 시퀀스로 변환하여 결과를 확인하는 것이 편하겠죠. 주어진 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만들어볼게요. 함수를 만들 때, Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외하고 Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외하도록 만들 거예요.  \n",
    "\n",
    "Q. seq2text 함수처럼 요약문의 정수 시퀀스를 텍스트로 변환하는 seq2summary 함수 코드를 작성하세요.\n",
    "(힌트 : 요약문에는 sostoken과 eostoken을 고려해야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc75d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        # 패딩(0), 시작 토큰(sostoken), 종료 토큰(eostoken)을 제외\n",
    "        if (i != 0 and i != tar_word_to_index['sostoken'] and i != tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp.strip()\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e155b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : first hard time opening mix came cloud lumps admit tried packet car made travel right taste ok nothing exciting thought could taste tea honey mostly tasted sweet kool aid type taste excited find cannot tolerate artificial sweetner sweetner made sugar low cal sure going work overall bad exciting \n",
      "실제 요약 : better than aid\n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : first purchased variety pack product costco black cherry always flavor first son pretty much lives beverage happy likes better soda since bit healthier happy able get amazon subscribe save even pay shipping \n",
      "실제 요약 : favorite flavor\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : great training aid easy use puppy follow lickety anywhere shaking container gets running \n",
      "실제 요약 : dog loves it\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : use popcorn popcorn popper makes best popcorn everyone comes movie night says best popcorn ever tried kinds buy popcorn easy make since everything included one bag cut top dump highly recommend popcorn \n",
      "실제 요약 : best popcorn ever made\n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : surprised good tastes refreshing drink know drinking juice bunch additives nice enjoy bring places might things drink due diet restrictions finally carbonated healthy alternative \n",
      "실제 요약 : have\n",
      "예측 요약 :  great tasting\n",
      "\n",
      "\n",
      "원문 : favorite flavor cups received quickly ordering would absolutely order recommend others \n",
      "실제 요약 : great product\n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : good good australian licorice actually strawberry seeds \n",
      "실제 요약 : strawberry licorice\n",
      "예측 요약 :  good product\n",
      "\n",
      "\n",
      "원문 : purchased candy vacation candy store michigan cost per pound plus tax delicious wanted went looking internet pleasantly surprised find candy much reasonable price arrived fresher even better tasting offered several friends family members hit careful addictive \n",
      "실제 요약 : will be back for more\n",
      "예측 요약 :  great candy\n",
      "\n",
      "\n",
      "원문 : expecting something like surprised smooth subtle switch definitely good combination refreshment flavor love fact tast carbonated \n",
      "실제 요약 : smooth flavor\n",
      "예측 요약 :  nice flavor\n",
      "\n",
      "\n",
      "원문 : every months super center stock favorite bbq sauce years good purpose sauce \n",
      "실제 요약 : why does not walmart carry this\n",
      "예측 요약 :  great sauce\n",
      "\n",
      "\n",
      "원문 : always wanted try ghee never found locally ordered tried fry fresh amazed difference ordinary frying used past also keeps well right shelf glad tried \n",
      "실제 요약 : great for fish\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : cant go wrong greenies dogs love go nuts downside soft gets chewed quickly dont quite see improve dogs teeth gums amazon best prices far tax amazon prime far becomes cheapest place buy products \n",
      "실제 요약 : greenies\n",
      "예측 요약 :  dogs love greenies\n",
      "\n",
      "\n",
      "원문 : bottles perfect long trips traveling definitely worth extra cost powder added convenience used recent vacation hawaii perfect mix powdered formula clean bottles plane also waste formula since bottles single serving make sure get several nipples directly bottle going somewhere cannot wash reuse \n",
      "실제 요약 : perfect for long\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : big tea drinker wanted health benefits drink tea everyday cannot wait morning drink tea right combination light mint flavor make refreshing white tea stain teeth readily teas high antioxidants \n",
      "실제 요약 : great tea\n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : days easy find us made dog natural treat relatively low price treated year old westie long time healthy partner beef turkey strips gave zuke try yet soon opened bag knew dog would like look good smell good even like taste dog would like \n",
      "실제 요약 : not tasty for my dog\n",
      "예측 요약 :  dog loves it\n",
      "\n",
      "\n",
      "원문 : give dreamfields lasagna stars taste quality give method ordering stars since found product exact quantity less elsewhere ripoff \n",
      "실제 요약 : stars for but\n",
      "예측 요약 :  price\n",
      "\n",
      "\n",
      "원문 : really enjoy tea dinner light soothing love kuerig green tea even better \n",
      "실제 요약 : nice after dinner\n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : bought christmas gift husband big dr pepper fans frequently use dr pepper soda cooking saw bbq sauce thought perfect hand make major disappointment sweet acid like taste like dr pepper ingredients list remotely related dr pepper thing say good stuff bring heat save money looking \n",
      "실제 요약 : dr who\n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : want feel like tropical vacation easy make fun drink yum \n",
      "실제 요약 : perfect for those hot summer\n",
      "예측 요약 :  great\n",
      "\n",
      "\n",
      "원문 : like coffee strong pleased start days offerings bought trying new bold coffees available new keurig brewer see options husband liked dark magic double black diamond cups thank amazon making choice use \n",
      "실제 요약 : dark and satisfying\n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : delicious spicey sauce italian restaurant pasta italian sausage sauce better must italian food lover item shipped promptly issues great company business thank \n",
      "실제 요약 : delicious\n",
      "예측 요약 :  great sauce\n",
      "\n",
      "\n",
      "원문 : bought popcorn popper figured would try popcorn see please dont waste money stuff small tough thought maybe since first time using try case new popper awful threw popped popcorn even birds eat \n",
      "실제 요약 : worst popcorn have ever had\n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : love anything onions certainly great product flavor sweet tangy delicious many things tried plain chicken even hamburgers trying pork weekend great price hope always available amazon \n",
      "실제 요약 : absolutely delicious\n",
      "예측 요약 :  great flavor\n",
      "\n",
      "\n",
      "원문 : liked much subscribe save option bit bolder average cup good taste balanced flavors every traditional plastic cup rather mesh type thing holding coffee got stuck brewer minor big deal \n",
      "실제 요약 : another great cup\n",
      "예측 요약 :  good coffee but not good\n",
      "\n",
      "\n",
      "원문 : make lot hummus oil perfect noticed couple reviewers problems dented cans shipping shipment arrived appears someone paying attention feedback cans shrink wrapped together prevent shipping purchase product locally choose amazon website easy shipping program awesome prices competitive thanks amazon \n",
      "실제 요약 : good stuff\n",
      "예측 요약 :  damaged cans\n",
      "\n",
      "\n",
      "원문 : bread mix works well bread maker sweet best mix non gluten flour half half \n",
      "실제 요약 : good bread mix\n",
      "예측 요약 :  great bread\n",
      "\n",
      "\n",
      "원문 : coffee great like medium strength cup coffee like chocolate note end sure would taste using drip even french press use barely present end \n",
      "실제 요약 : dark roast but not bold\n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : dozens capsules hope would help may enough could say really help surprisingly every time milk volume \n",
      "실제 요약 : for sometimes helped sometimes not\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : one favorite kind sun chips zesty taste sharp bland right kick nice crisp texture health food sodium fat content much better snack foods chips work really well dip vegetables pre dinner snack shop price price printed oz bag buy local supermarket price amazon higher manufacturer suggested retail price \n",
      "실제 요약 : delicious chips\n",
      "예측 요약 :  healthy snack\n",
      "\n",
      "\n",
      "원문 : love coffee right amount flavor strong light several cups day enjoy everyone \n",
      "실제 요약 : green mountain coffee breakfast blend\n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : delish strong tasty cup joe price used buy coffee time costco knew good \n",
      "실제 요약 : great deal\n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : purchased great grandson needed milk breast feeding one told best ordered thriving must great stuff \n",
      "실제 요약 : baby only organic toddler dairy formula\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : love order carnation vanilla packets line dollar less stores also get days guys per packets saved order cannot much better \n",
      "실제 요약 : as described\n",
      "예측 요약 :  great price\n",
      "\n",
      "\n",
      "원문 : purchased trying raspberry jelly love flavors tasty uses real fruit sugar one best jelly ever fresh processed color jelly true fruit color thank amazon providing top line products \n",
      "실제 요약 : delicious strawberry jelly\n",
      "예측 요약 :  best brown sugar\n",
      "\n",
      "\n",
      "원문 : organic lavender rooibos tea delicious free tea natural source health minerals high level antioxidants low tried rooibos suggest try tea hint lavender infusion delightful also make wash tea acne problems \n",
      "실제 요약 : organic lavender rooibos tea\n",
      "예측 요약 :  good tea\n",
      "\n",
      "\n",
      "원문 : fast shipment packaged well love soup makes want go italian market definitely back highly recommend maui \n",
      "실제 요약 : italian soup\n",
      "예측 요약 :  great seasoning\n",
      "\n",
      "\n",
      "원문 : easy roll mix used bread machine first rise made rolls using muffin tins oven great taste easy instructions great customer service highly recommend \n",
      "실제 요약 : great rolls\n",
      "예측 요약 :  great bread\n",
      "\n",
      "\n",
      "원문 : would give learned packaged food easily made prepare described box top mixed italian herbs crushed red pepper pinch shredded parmesan look appetizing tastes great things usually keep around house grabbed whatever adds seconds prep process highly recommend willing add things like flavor \n",
      "실제 요약 : add your own flavor to make it\n",
      "예측 요약 :  great for\n",
      "\n",
      "\n",
      "원문 : look like lot arrived take much fill quart jar sprouts days sprout nearly sprouting rate wonderful pad thai chinese rolls \n",
      "실제 요약 : more than it looks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  good product\n",
      "\n",
      "\n",
      "원문 : favorite boxed meals easy prepare delicious past six months able find locally broke stomach heart thank goodness amazon stuff awesome \n",
      "실제 요약 : this is the best stuff\n",
      "예측 요약 :  great\n",
      "\n",
      "\n",
      "원문 : one year old lab pit bull mix chew rawhide day purchased two rawhide lasted month \n",
      "실제 요약 : great for chewers\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : ordered sea salt vinegar ounce packs received sea salt ones instead little disappointed though like sea salt flavor ones wanted try vinegar version \n",
      "실제 요약 : did not receive what was ordered\n",
      "예측 요약 :  too salty\n",
      "\n",
      "\n",
      "원문 : awesome flavor great texture right amount pepper give great tingle taste buds definitely one best real steak highly recommended \n",
      "실제 요약 : excellent\n",
      "예측 요약 :  best rub\n",
      "\n",
      "\n",
      "원문 : fed dogs science diet iams years grew concerned recalls food research found canidae uses high quality ingredients dogs like much iams science diet feel great eating healthier food \n",
      "실제 요약 : good healthy dog food\n",
      "예측 요약 :  great food\n",
      "\n",
      "\n",
      "원문 : buffalo chick time favorites meat tender taste extreme flavor problem jerky salt ton would consume one bag day choice body definitely buying future though delicious \n",
      "실제 요약 : love it but huge on sodium though\n",
      "예측 요약 :  great jerky\n",
      "\n",
      "\n",
      "원문 : would give five star rating except fact little spicy stick back throat fire kind spicy easy prepare large serving noodles cook perfection follow instructions even chunks real chicken good value money nice product hand quick meal would serve children though probably hot \n",
      "실제 요약 : very spicy\n",
      "예측 요약 :  tasty and easy to make\n",
      "\n",
      "\n",
      "원문 : picky shih tzu finally eating consistantly well brand dog food coat shinier love wellness brand \n",
      "실제 요약 : finally my eats\n",
      "예측 요약 :  dog loves it\n",
      "\n",
      "\n",
      "원문 : visited uk taste salad cream disappointed find amazon available sellers high price kept checking found today became available amazon subscription salad cream remind salad dressing tasted child bit sweet creamy makes ordinary salad extraordinary makes look forward eat vegetables \n",
      "실제 요약 : am so\n",
      "예측 요약 :  the best\n",
      "\n",
      "\n",
      "원문 : great product people wanting gluten free cookies goes together easily cookies delicious add walnuts pecans mixture like nuts tastes great \n",
      "실제 요약 : wonderful cookies\n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : let us say five us really enjoy swedish fish usually gone matter days however bought pack christmas still half pack left \n",
      "실제 요약 : these actually last in this house\n",
      "예측 요약 :  great product\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59d189",
   "metadata": {},
   "source": [
    "많은 결과가 출력이 되는데, 기존의 요약과는 다른 요약을 출력하면서도 원문의 내용을 담고 있는 의미 있는 요약들이 보이네요. 심지어 일부 요약의 경우에는 원문에 없던 단어를 사용해서 요약을 하기도 하고 있어요. 워드 임베딩과 RNN의 콜라보로 이뤄낸 신기한 성과네요!\n",
    "\n",
    "물론 슬프게도 그다지 좋지 않은 요약의 예도 꽤나 보이기도 하네요. 성능을 개선하기 위해서는 seq2seq와 어텐션의 자체의 조합을 좀 더 좋게 수정하는 방법도 있고, 빔 서치(beam search), 사전 훈련된 워드 임베딩(pre-trained word embedding), 또는 인코더 - 디코더 자체의 구조를 새로이 변경한 하는 트랜스포머(Transformer)와 같은 여러 개선 방안들이 존재합니다. 이런 방안들에 대해서도 향후 살펴보게 될 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d878e91",
   "metadata": {},
   "source": [
    "### 3-12. 추출적 요약 해보기\n",
    "앞서 seq2seq를 통해서 추상적 요약을 진행해봤어요. 그런데 텍스트 요약에는 추상적 요약 외에도 이미 본문에 존재하는 단어구, 문장을 뽑아서 요약으로 삼는 추출적 요약 방법도 있었죠.\n",
    "\n",
    "Q. 우리가 앞에서 seq2seq 모델을 사용하여 추상적 요약을 했습니다. 그렇다면 추출적 요약은 무엇일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30090e",
   "metadata": {},
   "source": [
    "패키지 Summa에서는 추출적 요약을 위한 모듈인 summarize를 제공하고 있어 아주 간단하게 실습을 해볼 수 있어요. 영화 매트릭스 시놉시스를 요약해보면서 summarize 사용법을 익혀볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4f0be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fa33ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0d814",
   "metadata": {},
   "source": [
    "Summa의 summarize()의 인자로 사용되는 값들에 대해서 알아볼게요.  \n",
    "\n",
    "text (str) : 요약할 테스트.  \n",
    "ratio (float, optional) – 요약문에서 원본에서 선택되는 문장 비율. 0~1 사이값  \n",
    "words (int or None, optional) – 출력에 포함할 단어 수.  \n",
    "만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시한다.  \n",
    "split (bool, optional) – True면 문장 list / False는 조인(join)된 문자열을 반환  \n",
    "  \n",
    "Summa의 summarize는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행해요. 그렇기 때문에 문장 구분이 되어있지 않은 원문을 바로 입력으로 넣을 수 있어요. 비율을 적게 주어서 요약문으로 선택되는 문장의 개수를 줄여볼게요. 원문의 0.005%만을 출력하도록 설정했어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7968326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a964fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "# 리스트로 출력 결과를 받고 싶다면 split 인자의 값을 True로 변경.\n",
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e22cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, words=50)) # 단어의 수로 요약문의 크기를 조절할 수도 있어요. 단어를 50개만 선택한 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d73e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
