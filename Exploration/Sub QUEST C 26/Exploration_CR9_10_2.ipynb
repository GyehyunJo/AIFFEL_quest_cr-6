{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da56bb0",
   "metadata": {},
   "source": [
    "## 사용 데이터셋 : NSMC\n",
    "- 링크 : https://github.com/e9t/nsmc\n",
    "- txt로 되어있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a5fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ef7d6",
   "metadata": {},
   "source": [
    "https://wikidocs.net/31379\n",
    "\n",
    "### Step 1. 데이터 수집하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8d5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSMC 데이터셋 다운로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# NSMC 데이터셋 URL\n",
    "train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "\n",
    "# 다운로드할 파일 경로 설정\n",
    "train_filename = \"ratings_train.txt\"\n",
    "test_filename = \"ratings_test.txt\"\n",
    "\n",
    "# 파일 다운로드\n",
    "urllib.request.urlretrieve(train_url, filename=train_filename)\n",
    "urllib.request.urlretrieve(test_url, filename=test_filename)\n",
    "\n",
    "print(\"NSMC 데이터셋 다운로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2d9d9",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기\n",
    "- NSMC 데이터셋에 맞게 전처리 함수를 수정. \n",
    "- NSMC는 영화 리뷰 텍스트(document)와 감정 레이블(label)로 이루어진 데이터셋이기 때문에, 질문-답변 형식이 아니라 리뷰와 레이블로 데이터를 다룰 필요 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a245cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아 더빙 . . 진짜 짜증나네요 목소리', '흠 . . . 포스터보고 초딩영화줄 . . . . 오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 . . 솔직히 재미는 없다 . . 평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 ! 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다']\n",
      "[0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 데이터 전처리하기\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t')  # 탭으로 구분된 파일\n",
    "    data = data.dropna(how='any')  # 결측값 제거\n",
    "    reviews = data['document'].apply(preprocess_sentence).tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    return reviews, labels\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 문장부호 앞뒤로 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    # 한글, 문장부호 제외 모든 문자 공백 처리\n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", r\" \", sentence)\n",
    "    # 양쪽 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# 데이터 로드\n",
    "train_reviews, train_labels = load_data('ratings_train.txt')\n",
    "test_reviews, test_labels = load_data('ratings_test.txt')\n",
    "\n",
    "# 데이터 확인\n",
    "print(train_reviews[:5])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e56b0",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a842b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40) (64,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 없는 경우 예외 처리\n",
    "if len(train_reviews) == 0 or len(train_labels) == 0:\n",
    "    raise ValueError(\"데이터가 비어 있습니다. 데이터 파일을 확인해주세요.\")\n",
    "\n",
    "# SubwordTextEncoder를 사용하여 데이터 토큰화\n",
    "subword_encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_reviews, target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "def tokenize_and_encode(sentences):\n",
    "    return [subword_encoder.encode(sentence) for sentence in sentences]\n",
    "\n",
    "reviews_tokenized = tokenize_and_encode(train_reviews)\n",
    "\n",
    "# 패딩 추가\n",
    "MAX_LENGTH = 40\n",
    "def pad_sequences(tokenized_sentences):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_sentences, maxlen=MAX_LENGTH, padding='post'\n",
    "    )\n",
    "\n",
    "reviews_padded = pad_sequences(reviews_tokenized)\n",
    "\n",
    "# Train 데이터셋 생성\n",
    "labels = np.array(train_labels)\n",
    "\n",
    "inputs = reviews_padded\n",
    "outputs = labels\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "train_data = train_data.shuffle(len(train_reviews)).batch(64)\n",
    "\n",
    "# 데이터셋 확인\n",
    "for batch_inputs, batch_labels in train_data.take(1):\n",
    "    print(batch_inputs.shape, batch_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f96ff",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기\n",
    "- 리뷰는 트랜스포머의 디코더 부분이 필요하지 않음.\n",
    "- 디코더 없이 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e1f747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 트랜스포머 인코더 기반 감성 분류 모델 구성하기\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, units, vocab_size, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(1000, d_model)  # 임의의 최대 길이 설정 (여기서는 1000)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.flatten = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')  # 감정 분류를 위한 출력 레이어\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(d_model)[np.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(tf.shape(self.embedding.weights)[-1], tf.float32))  # d_model에 해당하는 값을 사용\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, x, x, attention_mask=mask)\n",
    "            x = self.dropout(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305ea968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_encoder_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1051776   \n",
      "_________________________________________________________________\n",
      "multi_head_attention_2 (Mult multiple                  263808    \n",
      "_________________________________________________________________\n",
      "multi_head_attention_3 (Mult multiple                  263808    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  129       \n",
      "=================================================================\n",
      "Total params: 1,579,521\n",
      "Trainable params: 1,579,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 설정\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "units = 512\n",
    "vocab_size = subword_encoder.vocab_size + 2  # 추가로 start와 end 토큰 고려\n",
    "dropout = 0.1\n",
    "\n",
    "# 모델 초기화\n",
    "transformer_encoder = TransformerEncoder(num_layers, d_model, num_heads, units, vocab_size, dropout)\n",
    "\n",
    "# 모델 빌드 (입력 형태 지정)\n",
    "input_shape = (None, MAX_LENGTH)  # (배치 크기, 입력 시퀀스 길이)\n",
    "transformer_encoder.build(input_shape=input_shape)\n",
    "\n",
    "# 모델 요약 출력\n",
    "transformer_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195f55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 및 손실 함수 설정\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    return loss_object(real, pred)\n",
    "\n",
    "# 정확도 메트릭 정의\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cf8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 단계 정의\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer_encoder(inp, training=True)\n",
    "        loss = loss_function(tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer_encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer_encoder.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(tar, predictions)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8757bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100, Loss: 0.6968\n",
      "Epoch 1 Batch 200, Loss: 0.6943\n",
      "Epoch 1 Batch 300, Loss: 0.4877\n",
      "Epoch 1 Batch 400, Loss: 0.4661\n",
      "Epoch 1 Batch 500, Loss: 0.4818\n",
      "Epoch 1 Batch 600, Loss: 0.4471\n",
      "Epoch 1 Batch 700, Loss: 0.4681\n",
      "Epoch 1 Batch 800, Loss: 0.3590\n",
      "Epoch 1 Batch 900, Loss: 0.3710\n",
      "Epoch 1 Batch 1000, Loss: 0.4369\n",
      "Epoch 1 Batch 1100, Loss: 0.3817\n",
      "Epoch 1 Batch 1200, Loss: 0.4608\n",
      "Epoch 1 Batch 1300, Loss: 0.4018\n",
      "Epoch 1 Batch 1400, Loss: 0.4765\n",
      "Epoch 1 Batch 1500, Loss: 0.3299\n",
      "Epoch 1 Batch 1600, Loss: 0.2921\n",
      "Epoch 1 Batch 1700, Loss: 0.3587\n",
      "Epoch 1 Batch 1800, Loss: 0.4013\n",
      "Epoch 1 Batch 1900, Loss: 0.4315\n",
      "Epoch 1 Batch 2000, Loss: 0.3744\n",
      "Epoch 1 Batch 2100, Loss: 0.4908\n",
      "Epoch 1 Batch 2200, Loss: 0.3710\n",
      "Epoch 1 Batch 2300, Loss: 0.4209\n",
      "Epoch 1, Loss: 0.4308, Accuracy: 0.7868\n",
      "Epoch 2 Batch 100, Loss: 0.3309\n",
      "Epoch 2 Batch 200, Loss: 0.2886\n",
      "Epoch 2 Batch 300, Loss: 0.2261\n",
      "Epoch 2 Batch 400, Loss: 0.4042\n",
      "Epoch 2 Batch 500, Loss: 0.3477\n",
      "Epoch 2 Batch 600, Loss: 0.2728\n",
      "Epoch 2 Batch 700, Loss: 0.3059\n",
      "Epoch 2 Batch 800, Loss: 0.4419\n",
      "Epoch 2 Batch 900, Loss: 0.2664\n",
      "Epoch 2 Batch 1000, Loss: 0.2994\n",
      "Epoch 2 Batch 1100, Loss: 0.3129\n",
      "Epoch 2 Batch 1200, Loss: 0.4393\n",
      "Epoch 2 Batch 1300, Loss: 0.2469\n",
      "Epoch 2 Batch 1400, Loss: 0.2859\n",
      "Epoch 2 Batch 1500, Loss: 0.2543\n",
      "Epoch 2 Batch 1600, Loss: 0.3055\n",
      "Epoch 2 Batch 1700, Loss: 0.3246\n",
      "Epoch 2 Batch 1800, Loss: 0.2846\n",
      "Epoch 2 Batch 1900, Loss: 0.2605\n",
      "Epoch 2 Batch 2000, Loss: 0.2717\n",
      "Epoch 2 Batch 2100, Loss: 0.3500\n",
      "Epoch 2 Batch 2200, Loss: 0.3946\n",
      "Epoch 2 Batch 2300, Loss: 0.3925\n",
      "Epoch 2, Loss: 0.3306, Accuracy: 0.8549\n",
      "Epoch 3 Batch 100, Loss: 0.2193\n",
      "Epoch 3 Batch 200, Loss: 0.3198\n",
      "Epoch 3 Batch 300, Loss: 0.3955\n",
      "Epoch 3 Batch 400, Loss: 0.3431\n",
      "Epoch 3 Batch 500, Loss: 0.2927\n",
      "Epoch 3 Batch 600, Loss: 0.2979\n",
      "Epoch 3 Batch 700, Loss: 0.3250\n",
      "Epoch 3 Batch 800, Loss: 0.3451\n",
      "Epoch 3 Batch 900, Loss: 0.2144\n",
      "Epoch 3 Batch 1000, Loss: 0.3139\n",
      "Epoch 3 Batch 1100, Loss: 0.4978\n",
      "Epoch 3 Batch 1200, Loss: 0.3190\n",
      "Epoch 3 Batch 1300, Loss: 0.4243\n",
      "Epoch 3 Batch 1400, Loss: 0.3635\n",
      "Epoch 3 Batch 1500, Loss: 0.2843\n",
      "Epoch 3 Batch 1600, Loss: 0.3526\n",
      "Epoch 3 Batch 1700, Loss: 0.2356\n",
      "Epoch 3 Batch 1800, Loss: 0.3460\n",
      "Epoch 3 Batch 1900, Loss: 0.3546\n",
      "Epoch 3 Batch 2000, Loss: 0.3588\n",
      "Epoch 3 Batch 2100, Loss: 0.2177\n",
      "Epoch 3 Batch 2200, Loss: 0.2996\n",
      "Epoch 3 Batch 2300, Loss: 0.2982\n",
      "Epoch 3, Loss: 0.3180, Accuracy: 0.8615\n",
      "Epoch 4 Batch 100, Loss: 0.2536\n",
      "Epoch 4 Batch 200, Loss: 0.3087\n",
      "Epoch 4 Batch 300, Loss: 0.2781\n",
      "Epoch 4 Batch 400, Loss: 0.2836\n",
      "Epoch 4 Batch 500, Loss: 0.1567\n",
      "Epoch 4 Batch 600, Loss: 0.2487\n",
      "Epoch 4 Batch 700, Loss: 0.2769\n",
      "Epoch 4 Batch 800, Loss: 0.3551\n",
      "Epoch 4 Batch 900, Loss: 0.3539\n",
      "Epoch 4 Batch 1000, Loss: 0.4285\n",
      "Epoch 4 Batch 1100, Loss: 0.3186\n",
      "Epoch 4 Batch 1200, Loss: 0.3920\n",
      "Epoch 4 Batch 1300, Loss: 0.3835\n",
      "Epoch 4 Batch 1400, Loss: 0.1611\n",
      "Epoch 4 Batch 1500, Loss: 0.3715\n",
      "Epoch 4 Batch 1600, Loss: 0.2332\n",
      "Epoch 4 Batch 1700, Loss: 0.2875\n",
      "Epoch 4 Batch 1800, Loss: 0.2464\n",
      "Epoch 4 Batch 1900, Loss: 0.3773\n",
      "Epoch 4 Batch 2000, Loss: 0.1965\n",
      "Epoch 4 Batch 2100, Loss: 0.3118\n",
      "Epoch 4 Batch 2200, Loss: 0.2889\n",
      "Epoch 4 Batch 2300, Loss: 0.3290\n",
      "Epoch 4, Loss: 0.3106, Accuracy: 0.8644\n",
      "Epoch 5 Batch 100, Loss: 0.2856\n",
      "Epoch 5 Batch 200, Loss: 0.3629\n",
      "Epoch 5 Batch 300, Loss: 0.2230\n",
      "Epoch 5 Batch 400, Loss: 0.2448\n",
      "Epoch 5 Batch 500, Loss: 0.2214\n",
      "Epoch 5 Batch 600, Loss: 0.3582\n",
      "Epoch 5 Batch 700, Loss: 0.2429\n",
      "Epoch 5 Batch 800, Loss: 0.3133\n",
      "Epoch 5 Batch 900, Loss: 0.2868\n",
      "Epoch 5 Batch 1000, Loss: 0.3043\n",
      "Epoch 5 Batch 1100, Loss: 0.3373\n",
      "Epoch 5 Batch 1200, Loss: 0.3558\n",
      "Epoch 5 Batch 1300, Loss: 0.2273\n",
      "Epoch 5 Batch 1400, Loss: 0.2974\n",
      "Epoch 5 Batch 1500, Loss: 0.3491\n",
      "Epoch 5 Batch 1600, Loss: 0.3807\n",
      "Epoch 5 Batch 1700, Loss: 0.2147\n",
      "Epoch 5 Batch 1800, Loss: 0.2566\n",
      "Epoch 5 Batch 1900, Loss: 0.4717\n",
      "Epoch 5 Batch 2000, Loss: 0.2716\n",
      "Epoch 5 Batch 2100, Loss: 0.1873\n",
      "Epoch 5 Batch 2200, Loss: 0.2944\n",
      "Epoch 5 Batch 2300, Loss: 0.2752\n",
      "Epoch 5, Loss: 0.3043, Accuracy: 0.8679\n",
      "Epoch 6 Batch 100, Loss: 0.2587\n",
      "Epoch 6 Batch 200, Loss: 0.2713\n",
      "Epoch 6 Batch 300, Loss: 0.3017\n",
      "Epoch 6 Batch 400, Loss: 0.2163\n",
      "Epoch 6 Batch 500, Loss: 0.2918\n",
      "Epoch 6 Batch 600, Loss: 0.2415\n",
      "Epoch 6 Batch 700, Loss: 0.3415\n",
      "Epoch 6 Batch 800, Loss: 0.2638\n",
      "Epoch 6 Batch 900, Loss: 0.2946\n",
      "Epoch 6 Batch 1000, Loss: 0.3732\n",
      "Epoch 6 Batch 1100, Loss: 0.3679\n",
      "Epoch 6 Batch 1200, Loss: 0.2218\n",
      "Epoch 6 Batch 1300, Loss: 0.2540\n",
      "Epoch 6 Batch 1400, Loss: 0.3129\n",
      "Epoch 6 Batch 1500, Loss: 0.2921\n",
      "Epoch 6 Batch 1600, Loss: 0.3284\n",
      "Epoch 6 Batch 1700, Loss: 0.2657\n",
      "Epoch 6 Batch 1800, Loss: 0.2425\n",
      "Epoch 6 Batch 1900, Loss: 0.4024\n",
      "Epoch 6 Batch 2000, Loss: 0.2543\n",
      "Epoch 6 Batch 2100, Loss: 0.2181\n",
      "Epoch 6 Batch 2200, Loss: 0.4334\n",
      "Epoch 6 Batch 2300, Loss: 0.4508\n",
      "Epoch 6, Loss: 0.2975, Accuracy: 0.8710\n",
      "Epoch 7 Batch 100, Loss: 0.3733\n",
      "Epoch 7 Batch 200, Loss: 0.3763\n",
      "Epoch 7 Batch 300, Loss: 0.3212\n",
      "Epoch 7 Batch 400, Loss: 0.3381\n",
      "Epoch 7 Batch 500, Loss: 0.2454\n",
      "Epoch 7 Batch 600, Loss: 0.3226\n",
      "Epoch 7 Batch 700, Loss: 0.2190\n",
      "Epoch 7 Batch 800, Loss: 0.3542\n",
      "Epoch 7 Batch 900, Loss: 0.3318\n",
      "Epoch 7 Batch 1000, Loss: 0.2774\n",
      "Epoch 7 Batch 1100, Loss: 0.2404\n",
      "Epoch 7 Batch 1200, Loss: 0.2764\n",
      "Epoch 7 Batch 1300, Loss: 0.3527\n",
      "Epoch 7 Batch 1400, Loss: 0.3550\n",
      "Epoch 7 Batch 1500, Loss: 0.3045\n",
      "Epoch 7 Batch 1600, Loss: 0.3649\n",
      "Epoch 7 Batch 1700, Loss: 0.2447\n",
      "Epoch 7 Batch 1800, Loss: 0.3651\n",
      "Epoch 7 Batch 1900, Loss: 0.4428\n",
      "Epoch 7 Batch 2000, Loss: 0.2110\n",
      "Epoch 7 Batch 2100, Loss: 0.2378\n",
      "Epoch 7 Batch 2200, Loss: 0.3673\n",
      "Epoch 7 Batch 2300, Loss: 0.3495\n",
      "Epoch 7, Loss: 0.2901, Accuracy: 0.8750\n",
      "Epoch 8 Batch 100, Loss: 0.2155\n",
      "Epoch 8 Batch 200, Loss: 0.2149\n",
      "Epoch 8 Batch 300, Loss: 0.1520\n",
      "Epoch 8 Batch 400, Loss: 0.2674\n",
      "Epoch 8 Batch 500, Loss: 0.1673\n",
      "Epoch 8 Batch 600, Loss: 0.2105\n",
      "Epoch 8 Batch 700, Loss: 0.3659\n",
      "Epoch 8 Batch 800, Loss: 0.2873\n",
      "Epoch 8 Batch 900, Loss: 0.2896\n",
      "Epoch 8 Batch 1000, Loss: 0.3333\n",
      "Epoch 8 Batch 1100, Loss: 0.2534\n",
      "Epoch 8 Batch 1200, Loss: 0.4017\n",
      "Epoch 8 Batch 1300, Loss: 0.3466\n",
      "Epoch 8 Batch 1400, Loss: 0.2799\n",
      "Epoch 8 Batch 1500, Loss: 0.3261\n",
      "Epoch 8 Batch 1600, Loss: 0.3585\n",
      "Epoch 8 Batch 1700, Loss: 0.2649\n",
      "Epoch 8 Batch 1800, Loss: 0.2033\n",
      "Epoch 8 Batch 1900, Loss: 0.3279\n",
      "Epoch 8 Batch 2000, Loss: 0.3405\n",
      "Epoch 8 Batch 2100, Loss: 0.2519\n",
      "Epoch 8 Batch 2200, Loss: 0.2628\n",
      "Epoch 8 Batch 2300, Loss: 0.2386\n",
      "Epoch 8, Loss: 0.2821, Accuracy: 0.8780\n",
      "Epoch 9 Batch 100, Loss: 0.3051\n",
      "Epoch 9 Batch 200, Loss: 0.2111\n",
      "Epoch 9 Batch 300, Loss: 0.2691\n",
      "Epoch 9 Batch 400, Loss: 0.3220\n",
      "Epoch 9 Batch 500, Loss: 0.3207\n",
      "Epoch 9 Batch 600, Loss: 0.2153\n",
      "Epoch 9 Batch 700, Loss: 0.2448\n",
      "Epoch 9 Batch 800, Loss: 0.4135\n",
      "Epoch 9 Batch 900, Loss: 0.3247\n",
      "Epoch 9 Batch 1000, Loss: 0.3099\n",
      "Epoch 9 Batch 1100, Loss: 0.2787\n",
      "Epoch 9 Batch 1200, Loss: 0.3116\n",
      "Epoch 9 Batch 1300, Loss: 0.2623\n",
      "Epoch 9 Batch 1400, Loss: 0.4212\n",
      "Epoch 9 Batch 1500, Loss: 0.4995\n",
      "Epoch 9 Batch 1600, Loss: 0.2610\n",
      "Epoch 9 Batch 1700, Loss: 0.3436\n",
      "Epoch 9 Batch 1800, Loss: 0.4113\n",
      "Epoch 9 Batch 1900, Loss: 0.3867\n",
      "Epoch 9 Batch 2000, Loss: 0.2618\n",
      "Epoch 9 Batch 2100, Loss: 0.2191\n",
      "Epoch 9 Batch 2200, Loss: 0.3867\n",
      "Epoch 9 Batch 2300, Loss: 0.2739\n",
      "Epoch 9, Loss: 0.2750, Accuracy: 0.8814\n",
      "Epoch 10 Batch 100, Loss: 0.2663\n",
      "Epoch 10 Batch 200, Loss: 0.2218\n",
      "Epoch 10 Batch 300, Loss: 0.3039\n",
      "Epoch 10 Batch 400, Loss: 0.2201\n",
      "Epoch 10 Batch 500, Loss: 0.3648\n",
      "Epoch 10 Batch 600, Loss: 0.2451\n",
      "Epoch 10 Batch 700, Loss: 0.2520\n",
      "Epoch 10 Batch 800, Loss: 0.2918\n",
      "Epoch 10 Batch 900, Loss: 0.2410\n",
      "Epoch 10 Batch 1000, Loss: 0.3749\n",
      "Epoch 10 Batch 1100, Loss: 0.2341\n",
      "Epoch 10 Batch 1200, Loss: 0.3744\n",
      "Epoch 10 Batch 1300, Loss: 0.3327\n",
      "Epoch 10 Batch 1400, Loss: 0.2660\n",
      "Epoch 10 Batch 1500, Loss: 0.3322\n",
      "Epoch 10 Batch 1600, Loss: 0.3786\n",
      "Epoch 10 Batch 1700, Loss: 0.2100\n",
      "Epoch 10 Batch 1800, Loss: 0.2242\n",
      "Epoch 10 Batch 1900, Loss: 0.3005\n",
      "Epoch 10 Batch 2000, Loss: 0.2606\n",
      "Epoch 10 Batch 2100, Loss: 0.2806\n",
      "Epoch 10 Batch 2200, Loss: 0.1758\n",
      "Epoch 10 Batch 2300, Loss: 0.1646\n",
      "Epoch 10, Loss: 0.2677, Accuracy: 0.8853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 학습\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_data):\n",
    "        # 타깃 데이터에서 예측해야 할 실젯값\n",
    "        tar = tf.expand_dims(tar, axis=1)  # 타깃 레이블이 (batch_size,)에서 (batch_size, 1) 형태로 변경됨\n",
    "        \n",
    "        # 배치 학습 실행\n",
    "        batch_loss = train_step(inp, tar)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # 배치 학습 중간 결과 출력 (옵션)\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch + 1}, Loss: {batch_loss:.4f}')\n",
    "\n",
    "    # 에포크 마다 손실 및 정확도 출력\n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    avg_accuracy = train_accuracy.result()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e18e6",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a993352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 이 영화 정말 재미있었어요!\n",
      "예측 감정: 긍정, 점수: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: 모델 평가하기\n",
    "def evaluate(sentence):\n",
    "    # 입력 문장 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [subword_encoder.encode(sentence)]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # 예측 수행\n",
    "    predictions = transformer_encoder(inputs, training=False)\n",
    "    \n",
    "    # 확률 값을 기준으로 긍정/부정 결정\n",
    "    score = predictions.numpy()[0][0]  # sigmoid 결과는 [0, 1] 사이의 값으로 출력됨\n",
    "    sentiment = '긍정' if score >= 0.5 else '부정'\n",
    "\n",
    "    return sentiment, score\n",
    "\n",
    "# 평가 예시\n",
    "sentence = \"이 영화 정말 재미있었어요!\"\n",
    "sentiment, score = evaluate(sentence)\n",
    "print(f\"문장: {sentence}\")\n",
    "print(f\"예측 감정: {sentiment}, 점수: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d75469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 이 영화 정말 재미있었어요!\n",
      "Bot: 감정: 긍정, 점수: 1.0000\n",
      "User: 완전 지루하고 별로였어요.\n",
      "Bot: 감정: 부정, 점수: 0.0065\n",
      "User: 배우들의 연기는 훌륭했지만, 스토리는 아쉬웠습니다.\n",
      "Bot: 감정: 긍정, 점수: 0.7499\n",
      "User: 다시 보고 싶지 않을 정도로 최악이었어요.\n",
      "Bot: 감정: 부정, 점수: 0.0028\n",
      "User: 정말 감동적이고 눈물이 났습니다.\n",
      "Bot: 감정: 긍정, 점수: 0.9956\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장 예측\n",
    "def predict(sentence):\n",
    "    sentiment, score = evaluate(sentence)\n",
    "    print(f'User: {sentence}')\n",
    "    print(f'Bot: 감정: {sentiment}, 점수: {score:.4f}')\n",
    "\n",
    "# 대화 예시\n",
    "predict(\"이 영화 정말 재미있었어요!\")\n",
    "predict(\"완전 지루하고 별로였어요.\")\n",
    "predict(\"배우들의 연기는 훌륭했지만, 스토리는 아쉬웠습니다.\")\n",
    "predict(\"다시 보고 싶지 않을 정도로 최악이었어요.\")\n",
    "predict(\"정말 감동적이고 눈물이 났습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90129e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
